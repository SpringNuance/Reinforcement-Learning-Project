{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "from types import SimpleNamespace as SN\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import utils.common_utils as cu\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from utils.recorder import RecordVideo\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to test a trained policy\n",
    "def test(agent, env_name, algo_name):\n",
    "    # Load model\n",
    "    agent.load_model()\n",
    "    print(\"Testing...\")\n",
    "    total_test_reward, total_test_len = 0, 0\n",
    "    returns = []\n",
    "    \n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo_name}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env_name}_env.yaml', 'r'))\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/env_cfg[\"env_name\"]/algo_name\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    \n",
    "    for ep in range(agent.cfg.test_episodes):\n",
    "        frames = []\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "        observation, _ = agent.env.reset(seed=seed)\n",
    "        test_reward, test_len, done = 0, 0, False\n",
    "        \n",
    "        while not done and test_len < agent.cfg.max_episode_steps:\n",
    "            action, _ = agent.get_action(observation, evaluation=True)\n",
    "            observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "            fs = agent.env.render()\n",
    "            frames = frames+fs\n",
    "            test_reward += reward\n",
    "            test_len += 1\n",
    "        total_test_reward += test_reward\n",
    "        total_test_len += test_len\n",
    "        returns.append(test_reward)\n",
    "        \n",
    "        if ep%100==0:\n",
    "            cu.save_rgb_arrays_to_gif(frames, video_test_dir/('_seed_'+str(agent.seed)+'_ep_'+str(ep)+'.gif'))\n",
    "\n",
    "    print(f\"Average test reward over {len(returns)} episodes: {float(total_test_reward/agent.cfg.test_episodes)},+- {np.std(np.array(returns))}; \\\n",
    "        Average episode length: {total_test_len/agent.cfg.test_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: read the configurations and generate the environment.\n",
    "def setup(algo=None, env='easy', cfg_args={}, render=True, train_episodes=None):\n",
    "    # set the paths\n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    \n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env}_env.yaml', 'r'))\n",
    "    algo_cfg=yaml.safe_load(open(cfg_path /'algo'/f'{algo}.yaml', 'r'))\n",
    "    cfg.update(env_cfg)\n",
    "    cfg.update(algo_cfg)\n",
    "    cfg.update(cfg_args)\n",
    "    \n",
    "    # forcely change train_episodes\n",
    "    if train_episodes is None:\n",
    "        True\n",
    "    else:\n",
    "        cfg[\"train_episodes\"] = train_episodes\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/cfg[\"env_name\"]/str(algo)\n",
    "    model_dir=work_dir/\"model\"\n",
    "    logging_dir=work_dir/\"logging\"\n",
    "    video_train_dir=work_dir/\"video\"/\"train\"\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    for dir in [work_dir, model_dir, logging_dir, video_train_dir, video_test_dir]:\n",
    "        cu.make_dir(dir)\n",
    "        \n",
    "    cfg.update({'work_dir':work_dir, \"model_dir\":model_dir, \"logging_dir\": logging_dir, \"video_train_dir\": video_train_dir, \"video_test_dir\": video_test_dir})\n",
    "    cfg = SN(**cfg)\n",
    "    \n",
    "    # set seed\n",
    "    if cfg.seed == None:\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "    else:\n",
    "        seed = cfg.seed\n",
    "    \n",
    "    ## Create environment\n",
    "    env=cu.create_env(cfg_path /'envs'/f'{env}_env.yaml')\n",
    "\n",
    "   \n",
    "    if cfg.save_video:\n",
    "        # During testing, save every episode\n",
    "        if cfg.testing:\n",
    "            ep_trigger = 1\n",
    "            video_path = cfg.video_test_dir\n",
    "        # During training, save every 50th episode\n",
    "        else:\n",
    "            ep_trigger = 1000   # Save video every 50 episodes\n",
    "            video_path = cfg.video_train_dir\n",
    "        \n",
    "        if render:\n",
    "            env = RecordVideo(\n",
    "                env, video_path,\n",
    "                episode_trigger=lambda x: x % ep_trigger == 0,\n",
    "                name_prefix=cfg.exp_name)\n",
    "\n",
    "\n",
    "    eval_env=copy.deepcopy(env)\n",
    "    env.reset(seed=seed) # we only set the seed here. During training, we don't have to set the seed when performing reset().\n",
    "    eval_env.reset(seed=seed+1000)\n",
    "    eval_env=None # For simplicity, we don't evaluate the performance during training.\n",
    "        \n",
    "    # Get dimensionalities of actions and observations\n",
    "    action_space_dim = cu.get_space_dim(env.action_space)\n",
    "    observation_space_dim = cu.get_space_dim(env.observation_space)\n",
    "    \n",
    "    config={\n",
    "        \"args\": cfg,\n",
    "        \"env\":env,\n",
    "        \"eval_env\":eval_env,\n",
    "        \"action_space_dim\": action_space_dim,\n",
    "        \"observation_space_dim\": observation_space_dim,\n",
    "        \"seed\":seed\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/utils/recorder.py:250: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_vector_env to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_vector_env` for environment variables or `env.get_wrapper_attr('is_vector_env')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:271: UserWarning: \u001b[33mWARN: RGB-array rendering should return a numpy array, got <class 'list'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/train/project-episode-0.mp4\n",
      "Episode 0 Step 20 finished. Average episode return: 0.0\n",
      "Episode 100 Step 2020 finished. Average episode return: -0.01\n",
      "Episode 200 Step 4020 finished. Average episode return: 0.07\n",
      "Episode 300 Step 6020 finished. Average episode return: -0.21\n",
      "Episode 400 Step 8020 finished. Average episode return: -0.1\n",
      "Episode 500 Step 10020 finished. Average episode return: 0.29\n",
      "Episode 600 Step 12020 finished. Average episode return: 0.33\n",
      "Episode 700 Step 14020 finished. Average episode return: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Choose either PPO or DDPG\n",
    "implemented_algo ='ddpg' #'ppo' or 'ddpg'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "# for environment in ['easy', 'middle', 'difficult']:\n",
    "environment = 'middle'\n",
    "training_seeds = []\n",
    "\n",
    "# Train the algorithm with a specific random seed.\n",
    "# In total, we train the algorithm with three random seeds [0, 1, 2].\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "    \n",
    "    # torch.manual_seed(config[\"seed\"])\n",
    "    # np.random.seed(config[\"seed\"])\n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "now start testing for environment middle  agent: ddpg  seed: 0\n",
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/model/model_parameters_0.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/test/_seed_0_ep_0.gif\n",
      "Average test reward over 10 episodes: 1.3,+- 0.6403124237432849;         Average episode length: 20.0\n",
      "\n",
      "\n",
      "\n",
      "now start testing for environment middle  agent: ddpg  seed: 1\n",
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/model/model_parameters_1.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/test/_seed_1_ep_0.gif\n",
      "Average test reward over 10 episodes: 1.0,+- 0.7745966692414834;         Average episode length: 20.0\n",
      "\n",
      "\n",
      "\n",
      "now start testing for environment middle  agent: ddpg  seed: 2\n",
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/model/model_parameters_2.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg/video/test/_seed_2_ep_0.gif\n",
      "Average test reward over 10 episodes: 0.7,+- 0.9;         Average episode length: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different Tasks with different difficulty levels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NOTE: Uncomment the algorithm you implemented\n",
    "implemented_algo ='ddpg' #'ddpg' or 'ppo'\n",
    "\n",
    "environment = 'middle'\n",
    "training_seeds = []\n",
    "\n",
    "# for each algorithm, we will test the agent trained with specific random seed\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "    print('\\n\\n\\nnow start testing for environment',environment,' agent:',implemented_algo,' seed:',i)\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/utils/recorder.py:250: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_vector_env to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_vector_env` for environment variables or `env.get_wrapper_attr('is_vector_env')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:271: UserWarning: \u001b[33mWARN: RGB-array rendering should return a numpy array, got <class 'list'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lgk1910/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-0.mp4\n",
      "Episode 0 Step 20 finished. Average episode return: 1.0\n",
      "Episode 100 Step 2020 finished. Average episode return: 0.0\n",
      "Episode 200 Step 4020 finished. Average episode return: 0.05\n",
      "Episode 300 Step 6020 finished. Average episode return: -0.06\n",
      "Episode 400 Step 8020 finished. Average episode return: -0.05\n",
      "Episode 500 Step 10020 finished. Average episode return: 0.14\n",
      "Episode 600 Step 12020 finished. Average episode return: 0.44\n",
      "Episode 700 Step 14020 finished. Average episode return: 0.48\n",
      "Episode 800 Step 16020 finished. Average episode return: 0.52\n",
      "Episode 900 Step 18020 finished. Average episode return: 0.43\n",
      "Moviepy - Building video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-1000.mp4.\n",
      "Moviepy - Writing video /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-1000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/train/project-episode-1000.mp4\n",
      "Episode 1000 Step 20020 finished. Average episode return: 0.64\n",
      "Episode 1100 Step 22020 finished. Average episode return: 0.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/training2.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/training2.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPlease use ppo or ddpg!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/training2.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Train the agent using selected algorithm    \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/training2.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/algos/ddpg_extension.py:215\u001b[0m, in \u001b[0;36mDDPGExtension.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m log_count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mtrain_episodes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39m# collect data and update the policy\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     train_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iteration()\n\u001b[1;32m    216\u001b[0m     train_info\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mepisodes\u001b[39m\u001b[39m'\u001b[39m: ep})\n\u001b[1;32m    217\u001b[0m     total_step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mtrain_info[\u001b[39m'\u001b[39m\u001b[39mepisode_length\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/algos/ddpg_extension.py:193\u001b[0m, in \u001b[0;36mDDPGExtension.train_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     obs \u001b[39m=\u001b[39m next_obs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    191\u001b[0m \u001b[39m# update the policy after one episode\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m#s = time.perf_counter()\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate()\n\u001b[1;32m    194\u001b[0m \u001b[39m#e = time.perf_counter()\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[39m# Return stats of training\u001b[39;00m\n\u001b[1;32m    197\u001b[0m info\u001b[39m.\u001b[39mupdate({\n\u001b[1;32m    198\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mepisode_length\u001b[39m\u001b[39m'\u001b[39m: timesteps,\n\u001b[1;32m    199\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mep_reward\u001b[39m\u001b[39m'\u001b[39m: reward_sum,\n\u001b[1;32m    200\u001b[0m             })\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/algos/ddpg_extension.py:67\u001b[0m, in \u001b[0;36mDDPGExtension.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_ptr \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_transition: \u001b[39m# update once we have enough data\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(update_iter):\n\u001b[0;32m---> 67\u001b[0m         info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update()\n\u001b[1;32m     69\u001b[0m \u001b[39m# update the buffer_head:\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_ptr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Implement your improved algorithm either in algo/ddpg_extension.py or algo/ppo_extension.py\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "# from algos.ddpg_extension_QR import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "\n",
    "implemented_algo = 'ddpg_extension'# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/model/model_parameters_0.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/test/_seed_0_ep_0.gif\n",
      "Average test reward over 10 episodes: 0.7,+- 0.45825756949558394;         Average episode length: 20.0\n",
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/model/model_parameters_1.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/test/_seed_1_ep_0.gif\n",
      "Average test reward over 10 episodes: 0.9,+- 0.5385164807134504;         Average episode length: 20.0\n",
      "model loaded: /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/model/model_parameters_2.pt\n",
      "Testing...\n",
      "Saved GIF to /Users/lgk1910/Library/CloudStorage/OneDrive-AaltoUniversity/Learning2/ReinforcementLearning/Project/GitHub/Reinforcement-Learning-Project/results/SandingEnvMiddle/ddpg_extension/video/test/_seed_2_ep_0.gif\n",
      "Average test reward over 10 episodes: 1.2,+- 0.6000000000000001;         Average episode length: 20.0\n"
     ]
    }
   ],
   "source": [
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "\n",
    "implemented_algo = 'ddpg_extension'# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
