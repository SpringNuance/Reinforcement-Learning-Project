{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba46938f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Project - Sanding Task </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching materials for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Learning Objectives</a>\n",
    "* <a href='#2.'> 2. Introduction </a>\n",
    "* <a href='#3.'> 3. Sanding Task </a>\n",
    "* <a href='#4.'> 4. Code Structure and Files </a>\n",
    "* <a href='#5.'> 5. Tasks </a>\n",
    "* <a href='#6.'> 6. Implementation requirements </a>\n",
    "* <a href='#7.'> 7. Evaluation / grading </a>\n",
    "* <a href='#8.'> 8. Start the project </a>\n",
    "* <a href='#9'> 9. Submission </a>\n",
    "* <a href='#10'> 10. Feedback  </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implement the basic PPO or DDPG algorithm. Create a new file called either 'ddpg.py' or 'ppo.py' in the folder 'algos'. Run the algorithm in all three sanding environments. Report the results, that is, training plots and test performances. (30 points)</a>\n",
    "\n",
    "<a href='#T2'><b>Student Task 2.</b> Extend DDPG/PPO to improve the performance based on the hints in Section 5. Note the instructions on the extensions in Section 5. Create a new file called either 'ddpg_extension.py' or 'ppo_extension.py' in folder 'algos'. Do the following: 1. The base algorithm's performance must be improved such that the agent succeeds in the moderate difficulty environment. Report the results (training plot and test performance) for all three environments; 2. Follow the provided structure such that it can be tested with function 'test(agent)'; 3. If you implemented multiple extensions analyze the difference in performance between the extensions . (40 points)</a>\n",
    "\n",
    "<a href='#Q1'><b>Student Question 1.</b> Explain how you extended PPO/DDPG and why in a maximum of 200 words. In addition, explain briefly in which parts of the source code the changes are (refer to file name and function names or lines of code). (30 points) </a>\n",
    "\n",
    "<a href='#T3'><b>Student Task 3.</b> This task gives bonus points to the project works that get highest performance in the difficult sanding environment. At the end of the course, we will use everyone's improved agent (please submit your pretrained weights) to run the competition on the most difficult sanding environment. Competitive grading: all projects are evaluated in the difficult environment for performance and put into ranking order. Top 10% of submitted projects get bonus points. Best performing project (100% ranked) gets 20 bonus points, 95% ranked gets 10 bonus points, 90% or lower ranked gets 0 bonus points. (+20 points) </a>\n",
    "\n",
    "**Total Points:** 100 (+20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2e19d-6a45-4472-94a7-cbe7f66c11fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color:lightcoral; padding:10px; border-radius:5px\">\n",
    "\n",
    "# <span style=\"color:white\">0. Group Information (IMPORTANT)</span>\n",
    "\n",
    "## <span style=\"color:white\">Please read the instructions and fill in your group information</span>\n",
    "\n",
    "- This project work is intended to be completed in groups of 2 students, who will share the same grade. If you are looking for a project partner, please join the project channel on Zulip and introduce yourself. \n",
    "\n",
    "- However, it is also acceptable to complete the project individually. \n",
    "\n",
    "- **Only one student** from each group should submit the project.\n",
    "\n",
    "- Please provide the **NAME (First Name Last Name)**, **Aalto Student ID**, and **Aalto User Name** of each group member. For example:\n",
    "    - Member 0: Jane Doe, 123456, janed5\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149679af-1b65-44b7-8e8a-5bda39706371",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color:lightcoral; padding:10px; border-radius:5px; margin-bottom: 10px\">\n",
    "\n",
    "# <span style=\"color:white\">### TODO: Fill in Your Group Information HERE</span>\n",
    "\n",
    "DOUBLE CLICK TO EDIT\n",
    "- **Member 1:**\n",
    "  - Name:\n",
    "  - Aalto Student ID:\n",
    "  - Aalto User Name:\n",
    "\n",
    "- **Member 2 (if applicable):**\n",
    "  - Name:\n",
    "  - Aalto Student ID:\n",
    "  - Aalto User Name:\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de82751-fa7f-43fd-be23-890898fb184c",
   "metadata": {},
   "source": [
    "# 1. Learning Objectives <a id='1.'></a>\n",
    "In the project work, students move to a more independent working style compared with the exercises. In the exercises, instructions and template code for reinforcement learning algorithms was provided. However, in the project work the students are given **a new task** that they need to solve using reinforcement learning methods discussed during the course and others. The students need to decide which method they will use (either PPO or DDPG), extend the method, and explain why. Students may take advantage of code that they have already developed in the exercises or which was part of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4535f3",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Introduction <a id='2.'></a>\n",
    "\n",
    "The goal of the project work is to optimize the behavior of a sanding robot using reinforcement learning such that the robot avoids already painted areas but sands areas that need sanding. We give now a general motivation and task description. [Section 3](#3-task-definition) provides a more detailed task definition.\n",
    "\n",
    "<center>\n",
    "<figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "  <img src=\"imgs/robot_sanding.png\" width=\"width:40%\"/>\n",
    "</figure>\n",
    "</center>\n",
    "<center>Figure 1: Illustration of a robot sanding a planar area. Image source: (https://www.youtube.com/watch?v=TG-3NZzoZiM)</center>\n",
    "\n",
    "The robot operates on a 2-D plane and should hit a set of \"sanding areas\" using the sanding tool equipped. At the same time the robot has to avoid specific \"painted areas\" which are also defined as part of the system state. The sanding areas and painted areas are defined as part of the state space. The robot gets a negative reward for hitting painted areas and a positive reward for sanding sanding areas.\n",
    "\n",
    "The idea is to optimize the behavior of the robot using reinforcement learning based on either the PPO or DDPG algorithm. You should extend PPO or DDPG such that you get higher performance in the more challenging versions of the sanding task. When answering questions and documenting source code the important thing is to clearly state which kind of methods and techniques you have used and especially **why** you have used those.\n",
    "\n",
    "[Section 3](#3-task-definition) defines the sanding task. [Section 4](#4-code-structure--files-a-id4a) shows the structure of the provided file directory. [Section 5](#5-requirements-possible-extensions-and-hints-a-id5a) discusses the **mandatory project requirements** and **possible extensions** to the basic PPO and DDPG algorithms. [Section 6](#6-evaluation--grading-a-id7a) describes how the project is evaluated and graded. In [Section 7](#7-source-code-a--7a), you will add your implementation, perform the tasks and answer questions. \n",
    "In [Section 8](#8-feedback-a-id8a), you can provide feedback on the project work. The feedback is important since this is the first year this project work is used on the course.\n",
    "\n",
    "Note that if you have a plan for the project work but are unsure whether the plan satisfies the project work requirements, ask the teaching assistants for advice on Zulip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51d07b",
   "metadata": {},
   "source": [
    "# 3. Sanding Task <a id='3.'></a>\n",
    "The primary objective in this project is to optimize a sanding robot's behavior, aiming to maximize the expected cumulative reward $ J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right] $. The simulator for the sanding task is provided in the \"sanding.py\" Python file. This section defines the sanding task. Please, see below.\n",
    "\n",
    "## 3.1. Markov Decision Process (MDP)\n",
    "- **Robot Characteristics**: The robot is visualized as a <span style=\"color:purple\">purple</span> circle with a radius of 10, operating on a 2D plane. The x and y coordinates range from -50 to 50.\n",
    "- **Sanding & No-Sanding Areas**: There are sanding (<span style=\"color:green\">green</span>) and no-sanding (<span style=\"color:red\">red</span>) areas, each with a radius of 10. Their configurations vary based on the task.\n",
    "  \n",
    "### 3.1.1. State Representation\n",
    "A state \\( s \\) is defined as:\n",
    "\n",
    "$s = [(x_{\\text{ROBOT}}, y_{\\text{ROBOT}}), (x_{\\text{SAND}}, y_{\\text{SAND}})_1, \\dots,\n",
    "    (x_{\\text{SAND}}, y_{\\text{SAND}})_N, (x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_1, \\dots,\n",
    "    (x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_M)]$\n",
    "\n",
    "- $N$ is the number of sanding areas (circles)\n",
    "- $M$ is the number of no-sanding areas (circles) \n",
    "- $(x_{\\text{ROBOT}}, y_{\\text{ROBOT}})$ : Robot's current location\n",
    "- $(x_{\\text{SAND}}, y_{\\text{SAND}})_i$: Location of the $i$th sanding area\n",
    "- $(x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_j$: Location of the $j$th no-sanding area\n",
    "\n",
    "### 3.1.2. Action Space\n",
    "\n",
    "An action $a$ consists of target coordinates for the robot:\n",
    "\n",
    "$a = (a_x, a_y) \\in \\mathbb{R}^2$\n",
    "\n",
    "$a_x, a_y$ selects the current target coordinates of the robot arm. A PD-controller~\\cite{X} trys to move the robot arm from the current coordinates $(x_{\\text{ROBOT}}, y_{\\text{ROBOT}})$ to the target coordinates $a_x, a_y$. You do not need to necessarily understand how exactly the PD-controller works but the controller may not always move the robot to the correct coordinates in one time step, and, it may also overshoot the target location. Please, see below for a visualization of this behavior.\n",
    "\n",
    "\n",
    "### 3.1.3. Reward definition\n",
    "\n",
    "The reward is defined as the number of sanding locations the robot touches minus the number of no-sanding locations the robot touches, that is,\n",
    "$r_t = $ number of sanded sanding locations - number of sanded no-sanding locations . \n",
    "\n",
    "The robot can only sand a sanding or no-sanding location once. All sanding and no-sanding locations that are touched by the robot will be moved outside the operating area, that is, those locations will be outside the operating area in the subsequent time step. \n",
    "\n",
    "\n",
    "## 3.2 Scenarios with different difficulty levels: <a id='3.1'></a>\n",
    "\n",
    "\n",
    "### Environment Breakdown\n",
    "#### Environment 1: Easy Environment \n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/easy_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "- **No-Sanding Spots**: 1 (<span style=\"color:red\">red</span>)\n",
    "- **Sanding Spots**: 1 (<span style=\"color:green\">green</span>)\n",
    "- **PD Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n",
    "\n",
    "#### Environment 2: Moderate Difficulty Environment \n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/middle_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "- **No-Sanding Spots**: 2 (<span style=\"color:red\">red</span>)\n",
    "- **Sanding Spots**: 2 (<span style=\"color:green\">green</span>)\n",
    "- **Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n",
    "\n",
    "#### Environment 3: Difficult Environment \n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/difficult_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "- **No-Sanding Spots**: 4 (<span style=\"color:red\">red</span>)\n",
    "- **Sanding Spots**: 4 (<span style=\"color:green\">green</span>)\n",
    "- **Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d32d1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# 4 Code Structure & Files <a id='4.'></a>\n",
    "\n",
    "```project.ipynb``` is the main file needed to be modified for this project, but you can also add other auxiliary files if needed.  \n",
    "```\n",
    "├───cfg\n",
    "│   ├───algo                       # Algorithm configurations\n",
    "│   │   ├───ddpg.yaml\n",
    "│   │   └───ppo.yaml\n",
    "│   │   ├───ddpg_extension.yaml\n",
    "│   │   └───ppo_extension.yaml\n",
    "│   ├───envs                       # Environment configurations\n",
    "│   │   ├───difficult_env.yaml \n",
    "│   │   ├───easy_env.yaml\n",
    "│   │   ├───env_example.py         # A python example of how to utilize the environment\n",
    "│   │   └───middle_env.yaml\n",
    "├───imgs\n",
    "│   ├───difficult_env.gif\n",
    "│   ├───easy_env.gif\n",
    "│   ├───middle_env.gif\n",
    "│   ├───robot_sanding.pdf\n",
    "│   └───robot_sanding.png\n",
    "├───utils\n",
    "│   ├───common_utils.py\n",
    "│   └───recorder.py\n",
    "├───algo\n",
    "│   ├───ddpg.py\n",
    "│   └───ppo.py\n",
    "│   ├───ddpg_extension.py\n",
    "│   └───ppo_extension.py\n",
    "├───project.ipynb\n",
    "└───sanding.py\n",
    "\n",
    "```\n",
    "\n",
    "## 4.1 Execution time <a id='4.1'></a>\n",
    "\n",
    "The training of DDPG/PPO may take more than 30 min depending on the server load. If you have problems with the training time, you can train DDPG/PPO locally on your computer or a server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b023f1910632940",
   "metadata": {},
   "source": [
    "# 5. Tasks <a id='5.'></a>\n",
    "\n",
    "## I. Tasks 1: use following algorithms to solve the robot sanding task. \n",
    "  - **PPO** (you can take code from ex1 as a basis)\n",
    "  - **DDPG** (you can take code from ex6 as a basis)\n",
    "\n",
    "Implementations from the exercises can be used and extended, otherwise please implement algorithms yourself. For learning purposes, you can look at existing implementations on the Internet.\n",
    "\n",
    "- We do not want to focus on hyperparameter or neural network architecture tuning. Therefore, use the following choices:\n",
    "  - For PPO, use the neural network policy configuration found in exercise 1 code.\n",
    "  - For DDPG, use the neural network policy and value function configuration found in exercise 6 code.\n",
    "  - <span style=\"color:red\">We provide the hyper-parameters in configuration files 'cfg/algo/*', DO NOT change the parameters there</span>\n",
    "\n",
    "- **No copying of code! Code should be original, written by yourself or taken from the exercises.**\n",
    "\n",
    "- You should extend or modify your basic PPO/DDPG algorithm. Below possible extensions are described but you can also come up with your own ones. For each modification or extension, describe in exactly two sentences the extension and refer to where the modifications can be found in the code (for example, file name and function name or line numbers).\n",
    "\n",
    "- More detailed instructions are given in the following sections. If you are not sure what you are allowed or not allowed to do, contact the TAs, preferably on Zulip so that also others may learn from the question.\n",
    "\n",
    "**Note: not following the requirements may lead to point deduction or rejection of the project work.**\n",
    "\n",
    "## II. Task 2: possible extensions to improve perfromance\n",
    "\n",
    "After the basic PPO/DDPG implementation, you shall try to do some technical improvements to improve the agent's performance. Below we list several possible extensions you can apply to improve the perfromance. \n",
    "\n",
    "**Note 1**: some of the suggested extensions require more effort than others. If you implement some of the \"easier\" extensions that require less effort than others such as \"Driving during training the log standard deviation of each dimension of the Gaussian policy from the original value to zero\", please implement multiple extensions.\n",
    "\n",
    "**Note 2**: You can also propose your found improvements, but you should also give the references.\n",
    "\n",
    "#### PPO\n",
    "- **Exploration**: Crucial in policy gradient methods. Options include:\n",
    "  - During training, linearly decrease the log standard deviation of each dimension of the Gaussian policy from the original value to zero.\n",
    "  - During training, linearly decrease the standard deviation of each dimension of the Gaussian policy from the original value to a small value.\n",
    "  - Adding an entropy bonus to the policy loss or reward function. The strength of the entropy bonus is typically controlled by a parameter $\\alpha$. To select $\\alpha$, you can, for example:\n",
    "    - Keep $\\alpha$ constant.\n",
    "    - Employ a schedule for $\\alpha$, for example, drive it from a high value (high exploration) to a low one (high exploitation) during training.\n",
    "    - Employ a schedule for a target entropy.\n",
    "- **Implementation Techniques**: Several techniques can impact PPO's performance, such as value normalization.\n",
    "- **Further Reading**:\n",
    "  1. [The 37 Implementation Details of Proximal Policy Optimization (Shengyi et al., 2022)](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)\n",
    "  2. [What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Andrychowicz et al., 2020)](https://arxiv.org/abs/2006.05990)\n",
    "\n",
    "#### DDPG\n",
    "- **Mitigating Value Overestimation**: \n",
    "  - Consider the Twin Delayed DDPG (TD3) algorithm to address overestimation bias and training instability. ([Fujimoto et al., 2018](https://arxiv.org/abs/1802.09477))\n",
    "- **Utilizing Distributional Critics**: \n",
    "  - Focus on the entire distribution of value functions for enhanced performance and stability.\n",
    "  - **QR-DDPG**: Provides robust value estimates through quantile regression. ([Dabney et al., 2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17184))\n",
    "  - **D4PG**: Combines Distributional Value Functions, Off-Policy Training, and Actor-Critic Methods.\n",
    "  - **IQN for DDPG**: Extends DDPG by representing the full quantile function for the value distribution. ([Dabney et al., 2018](http://proceedings.mlr.press/v80/dabney18a.html))\n",
    "  - **FQF**: Enhances distributional RL by learning both quantile values and their fractions. ([Yang et al., 2019](https://papers.nips.cc/paper/2019/hash/8fb134e0e6d44a4f95a8bb2d5b2cb1c4-Abstract.html))\n",
    "- **Enhancing Exploration**: \n",
    "  - Explore efficiently and avoid suboptimal solutions with strategies like:\n",
    "    - **Ornstein-Uhlenbeck Process**: Generates correlated noise, helpful in control tasks with inertia.\n",
    "    - **Intrinsic Curiosity Module (ICM)**: Encourages exploration through self-supervised prediction. ([Pathak et al., 2017](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/html/Pathak_Curiosity-Driven_Exploration_by_CVPR_2017_paper.html))\n",
    "    - **Random Network Distillation (RND)**: Generates intrinsic rewards based on prediction errors. ([Burda et al., 2018](https://arxiv.org/abs/1810.12894))\n",
    "    - *Additional Strategies*: Feel free to explore other methods.\n",
    "\n",
    "#### Model-Based RL\n",
    "- To improve the basic PPO or DDPG algorithm, you can integrate model learning ([Model-based RL survey](https://arxiv.org/abs/2206.09328)) into the training process, that is, learning a dynamics and reward model.\n",
    "  - **More data**: With a learned model you can generate more data (the 200k sample limit applies only to samples generated using the sanding simulator, not to samples generated using learned models).\n",
    "  - **Planning and Acting**: Combine planning with learning for more informed decisions. You can use, for example, the cross entropy method (CEM) introduced during the course for planning with the learned model.\n",
    "\n",
    "## III. Hints & Tips\n",
    "\n",
    "### a) Hints\n",
    "- Consider the sanding area's dimensions of 100 units in both width and height when sampling x, y coordinates.\n",
    "- Due to the multidimensional actions, when using a Gaussian policy, remember to use a multivariate Gaussian probability distribution, or, product of standard Gaussian distributions that corresponds to a multivariate Gaussian distribution with a diagonal covariance matrix.\n",
    "\n",
    "### b) Debugging Tips\n",
    "- To debug with a fixed seed, set `reset(seed=fixed_seed)` when resetting the environment each episode. This ensures a consistent initial position, making the task easier to learn. By default, training uses random seeds, and policies are evaluated likewise. Leaving `reset()` empty defaults to random seeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce5c40",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">6. Implementation requirements </span> <a id='6.'></a>\n",
    "\n",
    "### <span style=\"color:red\">Your implemented algorithm must be compatible with the below requirements; otherwise, the code will be considered invalid.</span>\n",
    "\n",
    "## I. Data Saving Format\n",
    "\n",
    "### a) Training Logs: \n",
    "During the training, your code shall create a CSV file as the training log.\n",
    "\n",
    "  - This log should output a CSV file with the following format:\n",
    "    ```\n",
    "    ,episode_length,ep_reward,episodes,total_step,average_return\n",
    "    0,20,0.0,99,2000,-0.13\n",
    "    1,20,0.0,199,4000,0.02\n",
    "    ...\n",
    "    ```\n",
    "  - The training log should be saved as:\n",
    "    `results/<environment name>/<algorithm name>/logging/logs_<seed number>.csv`\n",
    "    \n",
    "    For example:\n",
    "    `results/SandingEnvDifficult/ddpg/logging/logs_0.csv`\n",
    "    \n",
    "\n",
    "### b) Model Weights\n",
    "\n",
    "During/after training, the policy/critic weights should be saved in the path:\n",
    "\n",
    "  `results/<environment name>/<algorithm name>/model/model_parameters_<seed number>.pt`\n",
    "  \n",
    "  For example:\n",
    "  \n",
    "  `results/SandingEnvDifficult/ddpg/model/model_parameters_0.pt`\n",
    "\n",
    "### c) Videos: \n",
    "\n",
    "Training videos will automatically be saved under path `results/<environment name>/<algorithm name>/video/train`. Note that you do not need to implement this.\n",
    "\n",
    "\n",
    "\n",
    "## II. Visualization Plot Functions\n",
    "\n",
    "Ensure that your implemented algorithm is compatible with the functions located in `utils/common_utils.py`:\n",
    "\n",
    "- **Single Training Curve**: \n",
    "  - Function: `plot_reward(path, seed, env_name)`\n",
    "  - Description: Plots the training curve of a single algorithm, trained with a specific seed.\n",
    "  \n",
    "- **Multiple Training Curves**: \n",
    "  - Function: `plot_algorithm_training(path, seeds, env_name)`\n",
    "  - Description: Plots the training curves of a single algorithm, trained with multiple specific random seeds.\n",
    "  - Example: `seeds=[0,1,2]`\n",
    "  \n",
    "- **Comparison of Training Performances**: \n",
    "  - Function: `compare_algorithm_training(algo1, algo2, seeds)`\n",
    "  - Description: Given two configured algorithms/agents, this function will generate comparison plots of their training performances.\n",
    "\n",
    "\n",
    "## III. <span style=\"color:red\"> Configurations </span>\n",
    "\n",
    "We provide the hyperparameters in 'cfg/algo', use those configuration files to initialize your agent. Example refer to following code. You should not change any parameters there.\n",
    "\n",
    "**Usage Example**:\n",
    "```python\n",
    "config = setup(algo='ppo', env='middle')\n",
    "    config[\"seed\"] = 0\n",
    "    agent = PPOAgent(config)\n",
    "```\n",
    "\n",
    "## IV. <span style=\"color:red\"> Training implementation </span>\n",
    "\n",
    "To ensure compatibility with the setup function, your implemented algorithm must follow the specified protocol:\n",
    "\n",
    "- **Function**: `setup(algo=None, env='easy', cfg_args={})`\n",
    "  - **Purpose**: Used for setting up the configurations.\n",
    "  - **Usage Example**:\n",
    "    ```python\n",
    "    config = setup(algo='ppo', env='middle')\n",
    "    config[\"seed\"] = 0\n",
    "    agent = PPOAgent(config)\n",
    "    agent.train()\n",
    "    ```\n",
    "  - **Parameters**:\n",
    "    - `algo`: Specify the algorithm. Use either `'ppo'` or `'ddpg'`.\n",
    "    - `env`: Specify the environment. Options include `'easy'`, `'middle'`, or `'difficult'`.\n",
    "\n",
    "## V. <span style=\"color:red\"> Test implementation </span>\n",
    "\n",
    "To ensure compatibility with the setup function, your implemented algorithm must follow the specified protocol:\n",
    "\n",
    "- **Function**: `test`\n",
    "  - **Purpose**: Used for test the performance of the agent.\n",
    "  - **Usage Example**:\n",
    "    ```python\n",
    "    test(agent, env_name='easy', algo_name='ddpg')\n",
    "    ```\n",
    "\n",
    "## VI. <span style=\"color:red\"> Access </span>\n",
    "\n",
    "You shoudl not modify any files in paths:\n",
    "- 'cfg/'\n",
    "- 'utils/'\n",
    "- 'sanding.py'\n",
    "\n",
    "## VI. <span style=\"color:red\"> Other tips </span>\n",
    "\n",
    "1. If you choose DDPG based on ex6, you should be careful with function 'get_action()'. You should not use \n",
    "```python\n",
    "    if self.buffer_ptr < self.random_transition:\n",
    "``` \n",
    "if you want to evluate the agent's performance when using:\n",
    "```python\n",
    "test(agent, env_name='easy', algo_name='ddpg')\n",
    "```\n",
    "\n",
    "2. The standard deviation of gaussian policy in PPO is crucial for performance.\n",
    "\n",
    "### <span style=\"color:red\">Your implemented algorithm must be compatible with the above requirements; otherwise, the code will be considered invalid.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb37c5-a697-4d88-9f97-48d123db2983",
   "metadata": {},
   "source": [
    "# 7. Evaluation / Grading <a id='7.'></a>\n",
    "\n",
    "### Grading\n",
    "The general evaluation and grading process will take into account the following aspects:\n",
    "\n",
    "- In Tasks 1 and 2, the grading will be based on the performance comparison between the extended algorithms and their respective baselines. For Task 2 and Question 1, the grading will focus on the implemented extensions.\n",
    "  - The PPO baseline is derived from the code used in Exercise 1, featuring a Gaussian policy with an isotropic covariance matrix. (For more details, please google \"isotropic covariance matrix\"). The log standard deviation of the policy is linearly scheduled from the initial log standard deviation to zero during the training.\n",
    "  - The DDPG baseline originates from the code used in Exercise 6. The exploration noise added to the action follows a Gaussian distribution (mean is 0 and standard deviation is 0.3).\n",
    "\n",
    "- In Question 1, the evaluation will consider the created extensions, including the description and explanation of the extensions, why those extensions were chosen, and the source code for the extensions.\n",
    "- In Task 3, there will be a competitive grading approach: all projects will be evaluated based on their performance in the difficult Task 3 environment and then ranked. The top 10% of the submitted projects will receive bonus points. The best performing project (100% ranked) will receive 20 bonus points, 95% ranked will get 10 bonus points, and projects ranked 90% or lower will receive 0 bonus points.\n",
    "\n",
    "### Baseline Performance:\n",
    "\n",
    "#### For PPO:\n",
    "- PPO (Easy Environment): mean: 0.63, standard deviation: 0.18\n",
    "- PPO (Middle Environment): mean: 0.75, standard deviation: 0.32\n",
    "- PPO (Difficult Environment): mean: 0.83, standard deviation: 0.41\n",
    "\n",
    "#### For DDPG:\n",
    "- DDPG (Easy Environment): mean: 0.73, standard deviation: 0.28\n",
    "- DDPG (Middle Environment): mean: 0.81, standard deviation: 0.18\n",
    "- DDPG (Difficult Environment): mean: 0.76, standard deviation: 0.25\n",
    "\n",
    "\n",
    "# 8. Start the project <a id='8.'></a>\n",
    "\n",
    "This section contains below all the source code including your implementation and the tasks and question that need to be filled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa046e7",
   "metadata": {},
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Initialize the code</b> </h3>\n",
    "    Run the following section to start the task. DO NOT MODIFY THE CODE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61638d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np \n",
    "from types import SimpleNamespace as SN\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import utils.common_utils as cu\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from utils.recorder import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70bace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:36:27.520118741Z",
     "start_time": "2023-10-29T14:36:27.505961926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to test a trained policy\n",
    "def test(agent, env_name, algo_name):\n",
    "    # Load model\n",
    "    agent.load_model()\n",
    "    print(\"Testing...\")\n",
    "    total_test_reward, total_test_len = 0, 0\n",
    "    returns = []\n",
    "    \n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo_name}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env_name}_env.yaml', 'r'))\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/env_cfg[\"env_name\"]/algo_name\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    \n",
    "    for ep in range(agent.cfg.test_episodes):\n",
    "        frames = []\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "        observation, _ = agent.env.reset(seed=seed)\n",
    "        test_reward, test_len, done = 0, 0, False\n",
    "        \n",
    "        while not done and test_len < agent.cfg.max_episode_steps:\n",
    "            action, _ = agent.get_action(observation, evaluation=True)\n",
    "            observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "            fs = agent.env.render()\n",
    "            frames = frames+fs\n",
    "            test_reward += reward\n",
    "            test_len += 1\n",
    "        total_test_reward += test_reward\n",
    "        total_test_len += test_len\n",
    "        returns.append(test_reward)\n",
    "        \n",
    "        if ep%100==0:\n",
    "            cu.save_rgb_arrays_to_gif(frames, video_test_dir/('_seed_'+str(agent.seed)+'_ep_'+str(ep)+'.gif'))\n",
    "\n",
    "    print(f\"Average test reward over {len(returns)} episodes: {total_test_reward/agent.cfg.test_episodes},+- {np.std(np.array(returns))}; \\\n",
    "        Average episode length: {total_test_len/agent.cfg.test_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: read the configurations and generate the environment.\n",
    "def setup(algo=None, env='easy', cfg_args={}, render=True, train_episodes=None):\n",
    "    # set the paths\n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    \n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env}_env.yaml', 'r'))\n",
    "    algo_cfg=yaml.safe_load(open(cfg_path /'algo'/f'{algo}.yaml', 'r'))\n",
    "    cfg.update(env_cfg)\n",
    "    cfg.update(algo_cfg)\n",
    "    cfg.update(cfg_args)\n",
    "    \n",
    "    # forcely change train_episodes\n",
    "    if train_episodes is None:\n",
    "        True\n",
    "    else:\n",
    "        cfg[\"train_episodes\"] = train_episodes\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/cfg[\"env_name\"]/str(algo)\n",
    "    model_dir=work_dir/\"model\"\n",
    "    logging_dir=work_dir/\"logging\"\n",
    "    video_train_dir=work_dir/\"video\"/\"train\"\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    for dir in [work_dir, model_dir, logging_dir, video_train_dir, video_test_dir]:\n",
    "        cu.make_dir(dir)\n",
    "        \n",
    "    cfg.update({'work_dir':work_dir, \"model_dir\":model_dir, \"logging_dir\": logging_dir, \"video_train_dir\": video_train_dir, \"video_test_dir\": video_test_dir})\n",
    "    cfg = SN(**cfg)\n",
    "    \n",
    "    # set seed\n",
    "    if cfg.seed == None:\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "    else:\n",
    "        seed = cfg.seed\n",
    "    \n",
    "    ## Create environment\n",
    "    env=cu.create_env(cfg_path /'envs'/f'{env}_env.yaml')\n",
    "\n",
    "   \n",
    "    if cfg.save_video:\n",
    "        # During testing, save every episode\n",
    "        if cfg.testing:\n",
    "            ep_trigger = 1\n",
    "            video_path = cfg.video_test_dir\n",
    "        # During training, save every 50th episode\n",
    "        else:\n",
    "            ep_trigger = 1000   # Save video every 50 episodes\n",
    "            video_path = cfg.video_train_dir\n",
    "        \n",
    "        if render:\n",
    "            env = RecordVideo(\n",
    "                env, video_path,\n",
    "                episode_trigger=lambda x: x % ep_trigger == 0,\n",
    "                name_prefix=cfg.exp_name)\n",
    "\n",
    "\n",
    "    eval_env=copy.deepcopy(env)\n",
    "    env.reset(seed=seed) # we only set the seed here. During training, we don't have to set the seed when performing reset().\n",
    "    eval_env.reset(seed=seed+1000)\n",
    "    eval_env=None # For simplicity, we don't evaluate the performance during training.\n",
    "        \n",
    "    # Get dimensionalities of actions and observations\n",
    "    action_space_dim = cu.get_space_dim(env.action_space)\n",
    "    observation_space_dim = cu.get_space_dim(env.observation_space)\n",
    "    \n",
    "    config={\n",
    "        \"args\": cfg,\n",
    "        \"env\":env,\n",
    "        \"eval_env\":eval_env,\n",
    "        \"action_space_dim\": action_space_dim,\n",
    "        \"observation_space_dim\": observation_space_dim,\n",
    "        \"seed\":seed\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143e0a",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1</b> (30 points) </h3> \n",
    "    Implement the basic PPO or DDPG algorithm. Create a new file called either 'ddpg.py' or 'ppo.py' in the folder 'algos'. Run the algorithm in all three environments. Report the results here (training plot and test performance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1c0a7",
   "metadata": {},
   "source": [
    "## Task 1.1: Train each agents' performance \n",
    "\n",
    "- Implement your algorithm (either DDPG or PPO) in algo/ddpg.py or algo/ppo.py\n",
    "- After the implementation, train your algorithm with the following code\n",
    "    - Train the algorithm in all three environments\n",
    "    - The code will train the algorithm with 3 random seeds\n",
    "- Your code must be compatible with the following provided python code\n",
    "\n",
    "**Below, you will find an example of how to test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is provided only for debugging\n",
    "\n",
    "train_episodes = 1000  # Limit the number of training episode for a fast test\n",
    "\n",
    "config=setup(algo='ddpg', env='easy', train_episodes=train_episodes, render=False)\n",
    "\n",
    "config[\"seed\"] = 43\n",
    "\n",
    "\n",
    "if config[\"args\"].algo_name == 'ppo':\n",
    "    agent=PPOAgent(config)\n",
    "elif config[\"args\"].algo_name == 'ddpg':\n",
    "    agent=DDPGAgent(config)\n",
    "else:\n",
    "    raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "# Train the agent using the selected algorithm    \n",
    "agent.train()\n",
    "test(agent, 'easy', 'ddpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0bb92",
   "metadata": {},
   "source": [
    "**If everything is fine, we start the proper training now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed52e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different sanding environment versions with different difficulty levels\n",
    "\n",
    "# Choose either PPO or DDPG\n",
    "implemented_algo ='ddpg'#'ppo' or 'ddpg'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "    training_seeds = []\n",
    "    \n",
    "    # Train the algorithm with a specific random seed.\n",
    "    # In total, we train the algorithm with three random seeds [0, 1, 2].\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment,train_episodes=train_episodes)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Train the agent using selected algorithm    \n",
    "        agent.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025f9e9",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, RUN the above code to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f156a0a",
   "metadata": {},
   "source": [
    "## Task 1.2: Evaluate the Performance of Each Agent\n",
    "\n",
    "For each environment, the algorithm has been trained using three different random seeds, resulting in the generation of three distinct models for each algorithm. Our next step is to assess the performance of each model.\n",
    "\n",
    "- Execute the code below and document the performance of each model:\n",
    "  - Report the mean and standard deviation of the performance across the three random seeds.\n",
    " \n",
    "- Use the provided report format below, and input the values based on the results of your experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different Tasks with different difficulty levels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NOTE: Uncomment the algorithm you implemented\n",
    "implemented_algo ='ddpg' #'ddpg' or 'ppo'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "\n",
    "    training_seeds = []\n",
    "    \n",
    "    # for each algorithm, we will test the agent trained with specific random seed\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "        \n",
    "        print('\\n\\n\\nnow start testing for environment',environment,' agent:',implemented_algo,' seed:',i)\n",
    "        # Test the agent in the selected environment\n",
    "        test(agent, environment, implemented_algo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1dc0d",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "- PPO_Easy_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "   \n",
    "- PPO_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "- PPO_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_Easy_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "   \n",
    "- DDPG_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "- DDPG_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "\n",
    "<span style=\"color:darkblue\">\n",
    "\n",
    "**Note:** \n",
    "    \n",
    "    - **Your algorithm should demonstrate performance comparable to the baselines, otherwise the answer will be considered as invalid.** \n",
    "    - **We will also run the code to verify the performance.**\n",
    "\n",
    "#### PPO Baseline Performance:\n",
    "- PPO (Easy Environment): mean: 0.63, standard deviation: 0.18\n",
    "- PPO (Middle Environment): mean: 0.75, standard deviation: 0.32\n",
    "- PPO (Difficult Environment): mean: 0.83, standard deviation: 0.41\n",
    "\n",
    "#### DDPG Baseline Performance:\n",
    "- DDPG (Easy Environment): mean: 0.73, standard deviation: 0.28\n",
    "- DDPG (Middle Environment): mean: 0.81, standard deviation: 0.18\n",
    "- DDPG (Difficult Environment): mean: 0.76, standard deviation: 0.25\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e4f03",
   "metadata": {},
   "source": [
    "If you are curious about visualizing policy behaviours, you can run the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# The example of visualizing the saved test GIFs\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Display the GIF in Jupyter\n",
    "display(Image(filename=\"imgs/difficult_env.gif\"))  # Change the file path to display yours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ea1f3",
   "metadata": {},
   "source": [
    "## Task 1.3: Plot the algorithm's performance in each environment\n",
    "\n",
    "If all above code runs successfully, now we want to make a plot of the algorithm's training performance. You can run the code below to make plots. The training performance will look similar to this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvEasy.png\" alt=\"PPO Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvMiddle.png\" alt=\"PPO Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Middle</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvDifficult.png\" alt=\"PPO Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Difficult</figcaption>\n",
    "  </figure>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvEasy.png\" alt=\"DDPG Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvMiddle.png\" alt=\"DDPG Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Middle</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvDifficult.png\" alt=\"DDPG Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Difficult</figcaption>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "**Note**: You do not need to make the plots look exactly the same as shown above.  The following code generates 3 figures (1 algorithm x 3 environments). Please comment below the algorithm you did not implement.\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **PPO Easy**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **PPO Middle**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **PPO Difficult**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    " \n",
    " or\n",
    " \n",
    "- **DDPG Easy**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **DDPG Middle**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **DDPG Difficult**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvDifficult.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9279894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo ='ddpg' # 'ppo' or 'ddpg'\n",
    "\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "\n",
    "    training_seeds = []\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # plot the statistical training curves with specific random seeds\n",
    "    cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6663ed6",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, RUN the above code to make training plots for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5754f7",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3><b>Student Task 2</b> (40 points)</h3>\n",
    "    Your objective in this task is to enhance the performance of the DDPG/PPO algorithms, taking inspiration from the suggestions provided in Section 5.II. Carefully read the extension guidelines outlined in Section 5.II, and proceed to modify either 'ddpg_extension.py' or 'ppo_extension.py' located in the 'algos' folder. \n",
    "\n",
    "    1. You must elevate the base algorithm's performance to ensure the agent's success in the moderate difficulty environment (environment = 'middle'). \n",
    "    \n",
    "    2. Please document your results here, including the training plots and test performance.\n",
    "    \n",
    "    3. Adhere to the given structure to facilitate testing with 'setup' and 'test'  function.\n",
    "    \n",
    "    4. If you choose PPO, implement 2 extensions. If you opt for DDPG, implement at least 1 extension.\n",
    "    \n",
    "    5. In cases where multiple extensions are implemented, conduct a thorough analysis to discern the performance variations between the different extensions.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086909da",
   "metadata": {},
   "source": [
    "## Task 2.1: Enhance Your Chosen Algorithm\n",
    "\n",
    "### a) Overview\n",
    "Improve the performance of your selected reinforcement learning algorithm. Ensure that your implementations are properly documented and organized for clarity.\n",
    "\n",
    "### b) Implementation Details\n",
    "- **Algorithm Improvements**: Enhance your chosen algorithm.\n",
    "  - For PPO, implement **at least two extensions**.\n",
    "  - For DDPG, implement **at least one extension**.\n",
    "  - Ensure that the performance is noticeably improved.\n",
    "  - Place your implementations in the appropriate file:\n",
    "    - 'algo/ddpg_extension.py' for DDPG\n",
    "    - 'algo/ppo_extension.py' for PPO\n",
    "\n",
    "### c) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### d) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **middle-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### e) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5a520",
   "metadata": {},
   "source": [
    "**Train**: After implementing the improvement extensions, run the following code to train your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your improved algorithm either in algo/ddpg_extension.py or algo/ppo_extension.py\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "\n",
    "implemented_algo = ''# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0584b",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb0848",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "\n",
    "   \n",
    "- PPO_extension_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "\n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_extension_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c93abe",
   "metadata": {},
   "source": [
    "## Task 2.2: Plot improved algorithm performance \n",
    "\n",
    "### a) Display the plots:\n",
    "Display the training performance of your improved algorithm, similarly as in task 1.3\n",
    "\n",
    "### b) Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **improved Middle**: \n",
    "  - `results/SandingEnvMiddle/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo =# 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "\n",
    "training_seeds = [0,1,2]\n",
    "\n",
    "config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "config[\"seed\"] = 0\n",
    "\n",
    "agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# plot the statistical training curves with specific random seeds\n",
    "cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b022732",
   "metadata": {},
   "source": [
    "## Task 2.3: Comparison of Improved and Original Algorithm Performance\n",
    "\n",
    "### a) Display the Plots\n",
    "Display the training performance of both the improved and the original algorithms.\n",
    "\n",
    "We aim to compare the training performances of the original and improved algorithms. To achieve this, we will generate the following plots, which will highlight the sample efficiency and the agent's performance throughout the training process. Below are some figures comparing the performances of DDPG and PPO:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/middle_compare_ddpg_ppo.png\" alt=\"PPO Middle Environment\" width=\"540\"/>\n",
    "    <figcaption>PPO vs DDPG (Middle environment)</figcaption>\n",
    "  </figure>\n",
    "  \n",
    "</p>\n",
    "\n",
    "**Note**: The display does not need to exactly match the figures shown above. However, the code should generate a figure to compare the original algorithm with the improved algorithm.\n",
    "\n",
    "### b) Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Original vs Improved (Middle Environment)**: \n",
    "  - `results/SandingEnvMiddle/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvMiddle/compare_ppo_ppo_extension.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'middle'\n",
    "\n",
    "orgin_alo_name = # 'ddpg' or 'ppo'\n",
    "improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "\n",
    "config=setup(algo=orgin_alo_name, env=environment, render=False)\n",
    "origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "\n",
    "config=setup(algo=improved_alo_name, env=environment, render=False)\n",
    "improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# make the comparison plot\n",
    "cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ff0f5",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1</b> (30 points) </h3> \n",
    "    Explain how you extended PPO/DDPG and why in a maximum of 200 words. In addition, explain briefly in which parts of the source code the changes are (refer to file name and function names or lines of code).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4be73c52d18c3e",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956e37e",
   "metadata": {},
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3</b> (+20 points) </h3>\n",
    "    This task give bonus points to the project works that get highest performance in the difficult environment. At the end of the course, we will use everyone's improved agent (please submit your pretrained weights) to run the competition on the most difficult sanding environment. Competitive grading: all projects are evaluated in the difficult environment for performance and put into ranking order. Top 10% of submitted projects get bonus points. Best performing project (100% ranked) gets 20 bonus points, 95% ranked gets 10 bonus points, 90% or lower ranked get 0 bonus points.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50416d1f",
   "metadata": {},
   "source": [
    "## Task 3.1: Evaluate Your Improved Algorithm with difficult environment\n",
    "\n",
    "\n",
    "### a) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### b) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **difficult-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### c) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f06c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "# implement your improved algorithm either in algo/ddpg_extension.py or algo/ppo_extension.py\n",
    "\n",
    "implemented_algo = ''# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'difficult'\n",
    "\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379623a",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208bbf24",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "\n",
    "   \n",
    "- PPO_extension_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "\n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_extension_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9da1f8",
   "metadata": {},
   "source": [
    "## Task 3.2: Plot the Improved Algorithm's Performance \n",
    "\n",
    "#### Display the Plots\n",
    "Display the training performance of your improved algorithm, similar to what was done in Task 2.2.\n",
    "\n",
    "#### Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Improved Difficult**: \n",
    "  - `results/SandingEnvDifficult/ppo_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    "  \n",
    "  or\n",
    "  \n",
    "  - `results/SandingEnvDifficult/ddpg_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f83a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo =# 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'difficult'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "\n",
    "training_seeds = [0,1,2]\n",
    "\n",
    "config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "config[\"seed\"] = 0\n",
    "\n",
    "agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# plot the statistical training curves with specific random seeds\n",
    "cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd319d",
   "metadata": {},
   "source": [
    "## Task 3.3: Plot improved algorithm's and original's comparison performance\n",
    "\n",
    "### Display the plots:\n",
    "Display the training performance of your improvement algorithm, similarly as in task 2.3\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **Original vs Improved (difficult environment)**: \n",
    "  - `results/SandingEnvDifficult/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvDifficult/compare_ppo_ppo_extension.pdf`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'difficult'\n",
    "\n",
    "orgin_alo_name = # 'ddpg' or 'ppo'\n",
    "improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "\n",
    "config=setup(algo=orgin_alo_name, env=environment, render=False)\n",
    "origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "\n",
    "config=setup(algo=improved_alo_name, env=environment, render=False)\n",
    "improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# make the comparison plot\n",
    "cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb02ea-e912-46fa-9d35-e4869b07c28b",
   "metadata": {
    "tags": []
   },
   "source": [
    " ---\n",
    "## Submitting <a id='9'></a>\n",
    "Ensure that all tasks and questions are answered, and that the necessary plots are saved in the appropriate locations. Below is a list of the relevant plots and files that need to be submitted for the project work:\n",
    "\n",
    "### 1. Model Weights\n",
    "For each algorithm, you should have saved three model weights in the corresponding paths:\n",
    "\n",
    "`'results/(environment name)/(algorithm name)/model/model_parameters_(seed number).pt'`\n",
    "\n",
    "**Examples:**\n",
    "- For a DDPG agent trained with seed number 0 in the middle-level sanding environment:\n",
    "  - `'results/SandingEnvMiddle/ddpg/model/model_parameters_0.pt'`\n",
    "\n",
    "**Submission Checklist:**\n",
    "Ensure that each algorithm (ddpg, ppo, ddpg_extension, ppo_extension) has three sets of model weights (model_parameters_0, model_parameters_1, model_parameters_2) saved in the above paths.\n",
    "\n",
    "### 2. Individual Algorithm Plots\n",
    "\n",
    "You need to check that you have plotted the average training performances and the comparison plots for each algorithm. For submission, ensure the following figures are included:\n",
    "\n",
    "#### Task 1.3:\n",
    "\n",
    "- **PPO Easy**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **PPO Middle**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **PPO Difficult**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    "  \n",
    "  or\n",
    "  \n",
    "  \n",
    "- **DDPG Easy**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **DDPG Middle**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **DDPG Difficult**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    "\n",
    "\n",
    "#### Task 2.3:\n",
    "\n",
    "- **Improved agent for Middle-level environment**: \n",
    "  - `results/SandingEnvMiddle/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n",
    "#### Task 2.4:\n",
    "\n",
    "- **Original vs Improved (Middle-level Environment)**: \n",
    "  - `results/SandingEnvMiddle/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvMiddle/compare_ppo_ppo_extension.pdf`\n",
    "  \n",
    "#### Task 3.3:\n",
    "\n",
    "- **Improved Difficult-level environment**: \n",
    "  - `results/SandingEnvDifficult/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n",
    "#### Task 3.4:\n",
    "\n",
    "- **Original vs Improved (Difficult-level Environment)**: \n",
    "  - `results/SandingEnvDifficult/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvDifficult/compare_ppo_ppo_extension.pdf`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a07d0e-2b83-4e02-8e9b-c457fbb35485",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Feedback <a id='10'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58b944-9121-4827-a042-72b01ed21146",
   "metadata": {},
   "source": [
    "1) How much time did the project work members in total spend on the project work? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 35.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f3e51-b088-4c7d-8ad4-4eac14e6691c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f72584-ec47-492f-8d91-beabc1bbf10d",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a68775-ff72-40c5-b29b-51de6e6df98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing PPO or DDPG for the sanding task (35 points)\n",
    "T2 = None   # Extending PPO/DDPG to work on the easy and moderate difficulty tasks (45 points)\n",
    "T3 = None   # Extending PPO/DDPG to work on the difficult task (+20 points)\n",
    "Q1 = None   # Question 1 How did you extend PPO/DDPG and why? (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb542-b18b-453a-8ce2-b3bd72b43b4a",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668114c2-b143-4971-9f3f-dc18583aef8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing PPO or DDPG for the sanding task (35 points)\n",
    "T2 = None   # Extending PPO/DDPG to work on the easy and moderate difficulty tasks (45 points)\n",
    "T3 = None   # Extending PPO/DDPG to work on the difficult task (+20 points)\n",
    "Q1 = None   # Question 1 How did you extend PPO/DDPG and why? (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07196-619f-47d7-85be-6d2c2d5334d8",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - How difficult / time consuming was the project work? What was the most difficult part?\n",
    "    - What should be changed in the project work?\n",
    "    - What was the most useful / interesting part in the project work?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994fbd2-12d1-41ef-aa8f-16e4cc266fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
