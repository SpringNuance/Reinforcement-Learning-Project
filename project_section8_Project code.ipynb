{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Start the project <a id='8.'></a>\n",
    "\n",
    "This section contains below all the source code including your implementation and the tasks and question that need to be filled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Initialize the code</b> </h3>\n",
    "    Run the following section to start the task. DO NOT MODIFY THE CODE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np \n",
    "from types import SimpleNamespace as SN\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import utils.common_utils as cu\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from utils.recorder import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to test a trained policy\n",
    "def test(agent, env_name, algo_name):\n",
    "    # Load model\n",
    "    agent.load_model()\n",
    "    print(\"Testing...\")\n",
    "    total_test_reward, total_test_len = 0, 0\n",
    "    returns = []\n",
    "    \n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo_name}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env_name}_env.yaml', 'r'))\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/env_cfg[\"env_name\"]/algo_name\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    \n",
    "    for ep in range(agent.cfg.test_episodes):\n",
    "        frames = []\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "        observation, _ = agent.env.reset(seed=seed)\n",
    "        test_reward, test_len, done = 0, 0, False\n",
    "        \n",
    "        while not done and test_len < agent.cfg.max_episode_steps:\n",
    "            action, _ = agent.get_action(observation, evaluation=True)\n",
    "            observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "            fs = agent.env.render()\n",
    "            frames = frames+fs\n",
    "            test_reward += reward\n",
    "            test_len += 1\n",
    "        total_test_reward += test_reward\n",
    "        total_test_len += test_len\n",
    "        returns.append(test_reward)\n",
    "        \n",
    "        if ep%100==0:\n",
    "            cu.save_rgb_arrays_to_gif(frames, video_test_dir/('_seed_'+str(agent.seed)+'_ep_'+str(ep)+'.gif'))\n",
    "\n",
    "    print(f\"Average test reward over {len(returns)} episodes: {total_test_reward/agent.cfg.test_episodes},+- {np.std(np.array(returns))}; \\\n",
    "        Average episode length: {total_test_len/agent.cfg.test_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: read the configurations and generate the environment.\n",
    "def setup(algo=None, env='easy', cfg_args={}, render=True, train_episodes=None):\n",
    "    # set the paths\n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    \n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env}_env.yaml', 'r'))\n",
    "    algo_cfg=yaml.safe_load(open(cfg_path /'algo'/f'{algo}.yaml', 'r'))\n",
    "    cfg.update(env_cfg)\n",
    "    cfg.update(algo_cfg)\n",
    "    cfg.update(cfg_args)\n",
    "    \n",
    "    # forcely change train_episodes\n",
    "    if train_episodes is None:\n",
    "        True\n",
    "    else:\n",
    "        cfg[\"train_episodes\"] = train_episodes\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/cfg[\"env_name\"]/str(algo)\n",
    "    model_dir=work_dir/\"model\"\n",
    "    logging_dir=work_dir/\"logging\"\n",
    "    video_train_dir=work_dir/\"video\"/\"train\"\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    for dir in [work_dir, model_dir, logging_dir, video_train_dir, video_test_dir]:\n",
    "        cu.make_dir(dir)\n",
    "        \n",
    "    cfg.update({'work_dir':work_dir, \"model_dir\":model_dir, \"logging_dir\": logging_dir, \"video_train_dir\": video_train_dir, \"video_test_dir\": video_test_dir})\n",
    "    cfg = SN(**cfg)\n",
    "    \n",
    "    # set seed\n",
    "    if cfg.seed == None:\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "    else:\n",
    "        seed = cfg.seed\n",
    "    \n",
    "    ## Create environment\n",
    "    env=cu.create_env(cfg_path /'envs'/f'{env}_env.yaml')\n",
    "\n",
    "   \n",
    "    if cfg.save_video:\n",
    "        # During testing, save every episode\n",
    "        if cfg.testing:\n",
    "            ep_trigger = 1\n",
    "            video_path = cfg.video_test_dir\n",
    "        # During training, save every 50th episode\n",
    "        else:\n",
    "            ep_trigger = 1000   # Save video every 50 episodes\n",
    "            video_path = cfg.video_train_dir\n",
    "        \n",
    "        if render:\n",
    "            env = RecordVideo(\n",
    "                env, video_path,\n",
    "                episode_trigger=lambda x: x % ep_trigger == 0,\n",
    "                name_prefix=cfg.exp_name)\n",
    "\n",
    "\n",
    "    eval_env=copy.deepcopy(env)\n",
    "    env.reset(seed=seed) # we only set the seed here. During training, we don't have to set the seed when performing reset().\n",
    "    eval_env.reset(seed=seed+1000)\n",
    "    eval_env=None # For simplicity, we don't evaluate the performance during training.\n",
    "        \n",
    "    # Get dimensionalities of actions and observations\n",
    "    action_space_dim = cu.get_space_dim(env.action_space)\n",
    "    observation_space_dim = cu.get_space_dim(env.observation_space)\n",
    "    \n",
    "    config={\n",
    "        \"args\": cfg,\n",
    "        \"env\":env,\n",
    "        \"eval_env\":eval_env,\n",
    "        \"action_space_dim\": action_space_dim,\n",
    "        \"observation_space_dim\": observation_space_dim,\n",
    "        \"seed\":seed\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1</b> (30 points) </h3> \n",
    "    Implement the basic PPO or DDPG algorithm. Create a new file called either 'ddpg.py' or 'ppo.py' in the folder 'algos'. Run the algorithm in all three environments. Report the results here (training plot and test performance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Train each agents' performance \n",
    "\n",
    "- Implement your algorithm (either DDPG or PPO) in algo/ddpg.py or algo/ppo.py\n",
    "- After the implementation, train your algorithm with the following code\n",
    "    - Train the algorithm in all three environments\n",
    "    - The code will train the algorithm with 3 random seeds\n",
    "- Your code must be compatible with the following provided python code\n",
    "\n",
    "**Below, you will find an example of how to test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is provided only for debugging\n",
    "\n",
    "train_episodes = 1000  # Limit the number of training episode for a fast test\n",
    "\n",
    "config=setup(algo='ddpg', env='easy', train_episodes=train_episodes, render=False)\n",
    "\n",
    "config[\"seed\"] = 43\n",
    "\n",
    "\n",
    "if config[\"args\"].algo_name == 'ppo':\n",
    "    agent=PPOAgent(config)\n",
    "elif config[\"args\"].algo_name == 'ddpg':\n",
    "    agent=DDPGAgent(config)\n",
    "else:\n",
    "    raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "# Train the agent using the selected algorithm    \n",
    "agent.train()\n",
    "test(agent, 'easy', 'ddpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If everything is fine, we start the proper training now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different sanding environment versions with different difficulty levels\n",
    "\n",
    "# Choose either PPO or DDPG\n",
    "implemented_algo ='ddpg'#'ppo' or 'ddpg'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "    training_seeds = []\n",
    "    \n",
    "    # Train the algorithm with a specific random seed.\n",
    "    # In total, we train the algorithm with three random seeds [0, 1, 2].\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment,train_episodes=train_episodes)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Train the agent using selected algorithm    \n",
    "        agent.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, RUN the above code to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Evaluate the Performance of Each Agent\n",
    "\n",
    "For each environment, the algorithm has been trained using three different random seeds, resulting in the generation of three distinct models for each algorithm. Our next step is to assess the performance of each model.\n",
    "\n",
    "- Execute the code below and document the performance of each model:\n",
    "  - Report the mean and standard deviation of the performance across the three random seeds.\n",
    " \n",
    "- Use the provided report format below, and input the values based on the results of your experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different Tasks with different difficulty levels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NOTE: Uncomment the algorithm you implemented\n",
    "implemented_algo ='ddpg' #'ddpg' or 'ppo'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "\n",
    "    training_seeds = []\n",
    "    \n",
    "    # for each algorithm, we will test the agent trained with specific random seed\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "        \n",
    "        print('\\n\\n\\nnow start testing for environment',environment,' agent:',implemented_algo,' seed:',i)\n",
    "        # Test the agent in the selected environment\n",
    "        test(agent, environment, implemented_algo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "- PPO_Easy_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "   \n",
    "- PPO_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "- PPO_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_Easy_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "   \n",
    "- DDPG_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "- DDPG_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "    \n",
    "\n",
    "<span style=\"color:darkblue\">\n",
    "\n",
    "**Note:** \n",
    "    \n",
    "    - **Your algorithm should demonstrate performance comparable to the baselines, otherwise the answer will be considered as invalid.** \n",
    "    - **We will also run the code to verify the performance.**\n",
    "\n",
    "#### PPO Baseline Performance:\n",
    "- PPO (Easy Environment): mean: 0.63, standard deviation: 0.18\n",
    "- PPO (Middle Environment): mean: 0.75, standard deviation: 0.32\n",
    "- PPO (Difficult Environment): mean: 0.83, standard deviation: 0.41\n",
    "\n",
    "#### DDPG Baseline Performance:\n",
    "- DDPG (Easy Environment): mean: 0.73, standard deviation: 0.28\n",
    "- DDPG (Middle Environment): mean: 0.81, standard deviation: 0.18\n",
    "- DDPG (Difficult Environment): mean: 0.76, standard deviation: 0.25\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about visualizing policy behaviours, you can run the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# The example of visualizing the saved test GIFs\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Display the GIF in Jupyter\n",
    "display(Image(filename=\"imgs/difficult_env.gif\"))  # Change the file path to display yours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Plot the algorithm's performance in each environment\n",
    "\n",
    "If all above code runs successfully, now we want to make a plot of the algorithm's training performance. You can run the code below to make plots. The training performance will look similar to this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvEasy.png\" alt=\"PPO Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvMiddle.png\" alt=\"PPO Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Middle</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvDifficult.png\" alt=\"PPO Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Difficult</figcaption>\n",
    "  </figure>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvEasy.png\" alt=\"DDPG Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvMiddle.png\" alt=\"DDPG Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Middle</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvDifficult.png\" alt=\"DDPG Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Difficult</figcaption>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "**Note**: You do not need to make the plots look exactly the same as shown above.  The following code generates 3 figures (1 algorithm x 3 environments). Please comment below the algorithm you did not implement.\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **PPO Easy**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **PPO Middle**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **PPO Difficult**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    " \n",
    " or\n",
    " \n",
    "- **DDPG Easy**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **DDPG Middle**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "- **DDPG Difficult**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvDifficult.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo ='ddpg' # 'ppo' or 'ddpg'\n",
    "\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "for environment in ['easy', 'middle', 'difficult']:\n",
    "\n",
    "    training_seeds = []\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # plot the statistical training curves with specific random seeds\n",
    "    cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, RUN the above code to make training plots for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3><b>Student Task 2</b> (40 points)</h3>\n",
    "    Your objective in this task is to enhance the performance of the DDPG/PPO algorithms, taking inspiration from the suggestions provided in Section 5.II. Carefully read the extension guidelines outlined in Section 5.II, and proceed to modify either 'ddpg_extension.py' or 'ppo_extension.py' located in the 'algos' folder. \n",
    "\n",
    "    1. You must elevate the base algorithm's performance to ensure the agent's success in the moderate difficulty environment (environment = 'middle'). \n",
    "    \n",
    "    2. Please document your results here, including the training plots and test performance.\n",
    "    \n",
    "    3. Adhere to the given structure to facilitate testing with 'setup' and 'test'  function.\n",
    "    \n",
    "    4. If you choose PPO, implement 2 extensions. If you opt for DDPG, implement at least 1 extension.\n",
    "    \n",
    "    5. In cases where multiple extensions are implemented, conduct a thorough analysis to discern the performance variations between the different extensions.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Enhance Your Chosen Algorithm\n",
    "\n",
    "### a) Overview\n",
    "Improve the performance of your selected reinforcement learning algorithm. Ensure that your implementations are properly documented and organized for clarity.\n",
    "\n",
    "### b) Implementation Details\n",
    "- **Algorithm Improvements**: Enhance your chosen algorithm.\n",
    "  - For PPO, implement **at least two extensions**.\n",
    "  - For DDPG, implement **at least one extension**.\n",
    "  - Ensure that the performance is noticeably improved.\n",
    "  - Place your implementations in the appropriate file:\n",
    "    - 'algo/ddpg_extension.py' for DDPG\n",
    "    - 'algo/ppo_extension.py' for PPO\n",
    "\n",
    "### c) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### d) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **middle-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### e) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**: After implementing the improvement extensions, run the following code to train your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your improved algorithm either in algo/ddpg_extension.py or algo/ppo_extension.py\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "\n",
    "implemented_algo = ''# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "\n",
    "   \n",
    "- PPO_extension_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "\n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_extension_Middle_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Plot improved algorithm performance \n",
    "\n",
    "### a) Display the plots:\n",
    "Display the training performance of your improved algorithm, similarly as in task 1.3\n",
    "\n",
    "### b) Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **improved Middle**: \n",
    "  - `results/SandingEnvMiddle/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo =# 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "\n",
    "training_seeds = [0,1,2]\n",
    "\n",
    "config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "config[\"seed\"] = 0\n",
    "\n",
    "agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# plot the statistical training curves with specific random seeds\n",
    "cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Comparison of Improved and Original Algorithm Performance\n",
    "\n",
    "### a) Display the Plots\n",
    "Display the training performance of both the improved and the original algorithms.\n",
    "\n",
    "We aim to compare the training performances of the original and improved algorithms. To achieve this, we will generate the following plots, which will highlight the sample efficiency and the agent's performance throughout the training process. Below are some figures comparing the performances of DDPG and PPO:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/middle_compare_ddpg_ppo.png\" alt=\"PPO Middle Environment\" width=\"540\"/>\n",
    "    <figcaption>PPO vs DDPG (Middle environment)</figcaption>\n",
    "  </figure>\n",
    "  \n",
    "</p>\n",
    "\n",
    "**Note**: The display does not need to exactly match the figures shown above. However, the code should generate a figure to compare the original algorithm with the improved algorithm.\n",
    "\n",
    "### b) Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Original vs Improved (Middle Environment)**: \n",
    "  - `results/SandingEnvMiddle/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvMiddle/compare_ppo_ppo_extension.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'middle'\n",
    "\n",
    "orgin_alo_name = # 'ddpg' or 'ppo'\n",
    "improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "\n",
    "config=setup(algo=orgin_alo_name, env=environment, render=False)\n",
    "origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "\n",
    "config=setup(algo=improved_alo_name, env=environment, render=False)\n",
    "improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# make the comparison plot\n",
    "cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1</b> (30 points) </h3> \n",
    "    Explain how you extended PPO/DDPG and why in a maximum of 200 words. In addition, explain briefly in which parts of the source code the changes are (refer to file name and function names or lines of code).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3</b> (+20 points) </h3>\n",
    "    This task give bonus points to the project works that get highest performance in the difficult environment. At the end of the course, we will use everyone's improved agent (please submit your pretrained weights) to run the competition on the most difficult sanding environment. Competitive grading: all projects are evaluated in the difficult environment for performance and put into ranking order. Top 10% of submitted projects get bonus points. Best performing project (100% ranked) gets 20 bonus points, 95% ranked gets 10 bonus points, 90% or lower ranked get 0 bonus points.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Evaluate Your Improved Algorithm with difficult environment\n",
    "\n",
    "\n",
    "### a) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### b) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **difficult-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### c) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "# implement your improved algorithm either in algo/ddpg_extension.py or algo/ppo_extension.py\n",
    "\n",
    "implemented_algo = ''# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'difficult'\n",
    "\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Test the agent in the selected environment\n",
    "    test(agent, environment, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here**:\n",
    "\n",
    "\n",
    "   \n",
    "- PPO_extension_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    "\n",
    " or \n",
    " \n",
    " \n",
    "- DDPG_extension_Difficult_environment:\n",
    "    - mean:\n",
    "    - standard deviation:\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Plot the Improved Algorithm's Performance \n",
    "\n",
    "#### Display the Plots\n",
    "Display the training performance of your improved algorithm, similar to what was done in Task 2.2.\n",
    "\n",
    "#### Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Improved Difficult**: \n",
    "  - `results/SandingEnvDifficult/ppo_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    "  \n",
    "  or\n",
    "  \n",
    "  - `results/SandingEnvDifficult/ddpg_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Uncomment the algorithm you chose \n",
    "implemented_algo =# 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'difficult'\n",
    "\n",
    "# Loop over the three difficulty levels\n",
    "\n",
    "training_seeds = [0,1,2]\n",
    "\n",
    "config=setup(algo=implemented_algo, env=environment, render=False)\n",
    "\n",
    "config[\"seed\"] = 0\n",
    "\n",
    "agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# plot the statistical training curves with specific random seeds\n",
    "cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Plot improved algorithm's and original's comparison performance\n",
    "\n",
    "### Display the plots:\n",
    "Display the training performance of your improvement algorithm, similarly as in task 2.3\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **Original vs Improved (difficult environment)**: \n",
    "  - `results/SandingEnvDifficult/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvDifficult/compare_ppo_ppo_extension.pdf`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'difficult'\n",
    "\n",
    "orgin_alo_name = # 'ddpg' or 'ppo'\n",
    "improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "\n",
    "config=setup(algo=orgin_alo_name, env=environment, render=False)\n",
    "origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "\n",
    "config=setup(algo=improved_alo_name, env=environment, render=False)\n",
    "improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "# make the comparison plot\n",
    "cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
